{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1b64a6f6-1d32-48be-92b5-66c3b04b17f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU \\\n",
        "    langgraph==0.2.4 \\\n",
        "    langchain-openai==0.1.22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a917bb70-f84c-48e6-8d32-d14f9df2ca2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "567b6c4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a2ac853-b8a6-40de-b7fe-3f9f3c5ca4d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool(\"website_schema_validator_tool\")\n",
        "def website_schema_validator_tool(website_schema: str = None) -> bool:\n",
        "    \"\"\"\n",
        "    Validate a website schema\n",
        "\n",
        "    Args:\n",
        "    schema (str): The website schema to validate\n",
        "\n",
        "    Returns:\n",
        "    bool: Whether the schema is valid\n",
        "    \"\"\"\n",
        "    return True if website_schema else False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "5fffa8d5-068a-4f0b-adfc-b4daf30ef294",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
        "\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    missing: str = Field(description=\"Critique of what is missing.\")\n",
        "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
        "\n",
        "\n",
        "class AnswerQuestion(BaseModel):\n",
        "    \"\"\"Answer the question. Provide an answer, reflection, and then follow up with search queries to improve the answer.\"\"\"\n",
        "\n",
        "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
        "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
        "    search_queries: list[str] = Field(\n",
        "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class ResponderWithRetries:\n",
        "    def __init__(self, runnable, validator, name=\"\"):\n",
        "        self.runnable = runnable\n",
        "        self.validator = validator\n",
        "        self.name = name\n",
        "\n",
        "    def respond(self, state):\n",
        "        response = []\n",
        "        for attempt in range(3):\n",
        "            print(f\"Responder::{self.name} Attempt {attempt}\")\n",
        "            response = self.runnable.invoke(state, {\"tags\": [f\"attempt:{attempt}\"]})\n",
        "            try:\n",
        "                self.validator.invoke(response)\n",
        "                print(f\"Responder::{self.name} Response is valid\")\n",
        "                return response\n",
        "            except ValidationError as e:\n",
        "                print(f\"Responder::{self.name} Response is invalid\")\n",
        "                state[\"messages\"] = state[\"messages\"] + [\n",
        "                    response,\n",
        "                    ToolMessage(\n",
        "                        content=f\"{repr(e)}\\n\\nPay close attention to the function schema.\\n\\n\"\n",
        "                        + self.validator.schema_json()\n",
        "                        + \" Respond by fixing all validation errors.\",\n",
        "                        tool_call_id=response.tool_calls[0][\"id\"],\n",
        "                    ),\n",
        "                ]\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4a0264b8-ed2d-4f15-9d3c-085aa3a5edab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            \"\"\"You are expert researcher.\n",
        "Current time: {time}\n",
        "\n",
        "1. {first_instruction}\n",
        "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
        "3. Recommend search queries to research information and improve your answer.\"\"\",\n",
        "        ),\n",
        "        MessagesPlaceholder(\"messages\"),\n",
        "        HumanMessage(\n",
        "            \"\\n\\n<system>Reflect on the user's original question and the\"\n",
        "            \" actions taken thus far. Respond using the {function_name} function.</reminder>\",\n",
        "        ),\n",
        "    ]\n",
        ").partial(\n",
        "    time=lambda: datetime.datetime.now().isoformat(),\n",
        ")\n",
        "initial_answer_chain = actor_prompt_template.partial(\n",
        "    first_instruction=\"Provide a detailed ~250 word answer.\",\n",
        "    function_name=AnswerQuestion.__name__,\n",
        ") | llm.bind_tools(tools=[AnswerQuestion])\n",
        "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
        "\n",
        "first_responder = ResponderWithRetries(\n",
        "    runnable=initial_answer_chain, validator=validator, name=\"First\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "5922e1fe-7533-4f41-8b1d-d812707c1968",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt 0\n",
            "Response is invalid\n",
            "Attempt 1\n",
            "Response is valid\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ttsoi7ZvXc2ahV9m0JTnpFGU', 'function': {'arguments': '{\"answer\":\"Reflection is a crucial aspect of AI for several reasons. Firstly, it allows AI systems to evaluate their own performance and make adjustments to improve accuracy and efficiency. By reflecting on past actions and outcomes, AI can identify patterns and learn from mistakes, leading to continuous improvement. Secondly, reflection helps in debugging and troubleshooting. When an AI system encounters an error or produces an unexpected result, reflection can help trace back the steps and identify the root cause. Thirdly, reflection enhances transparency and accountability. By documenting the decision-making process, AI systems can provide explanations for their actions, which is essential for building trust with users. Lastly, reflection supports ethical AI development. By continuously assessing the impact of their actions, AI systems can ensure they are aligned with ethical guidelines and avoid harmful behaviors.\",\"reflection\":{\"missing\":\"The answer lacks specific examples of AI systems that use reflection and concrete evidence or studies that support the claims made. Including these would strengthen the argument.\",\"superfluous\":\"The answer is concise and to the point, with no superfluous information. Each point made is relevant to the question.\"},\"search_queries\":[\"examples of AI systems using reflection\",\"studies on reflection in AI\",\"importance of reflection in AI development\"]}', 'name': 'AnswerQuestion'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 255, 'prompt_tokens': 753, 'total_tokens': 1008}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fca8d500-cdfb-46d9-b105-819dbc730950-0', tool_calls=[{'name': 'AnswerQuestion', 'args': {'answer': 'Reflection is a crucial aspect of AI for several reasons. Firstly, it allows AI systems to evaluate their own performance and make adjustments to improve accuracy and efficiency. By reflecting on past actions and outcomes, AI can identify patterns and learn from mistakes, leading to continuous improvement. Secondly, reflection helps in debugging and troubleshooting. When an AI system encounters an error or produces an unexpected result, reflection can help trace back the steps and identify the root cause. Thirdly, reflection enhances transparency and accountability. By documenting the decision-making process, AI systems can provide explanations for their actions, which is essential for building trust with users. Lastly, reflection supports ethical AI development. By continuously assessing the impact of their actions, AI systems can ensure they are aligned with ethical guidelines and avoid harmful behaviors.', 'reflection': {'missing': 'The answer lacks specific examples of AI systems that use reflection and concrete evidence or studies that support the claims made. Including these would strengthen the argument.', 'superfluous': 'The answer is concise and to the point, with no superfluous information. Each point made is relevant to the question.'}, 'search_queries': ['examples of AI systems using reflection', 'studies on reflection in AI', 'importance of reflection in AI development']}, 'id': 'call_ttsoi7ZvXc2ahV9m0JTnpFGU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 753, 'output_tokens': 255, 'total_tokens': 1008})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_question = \"Why is reflection useful in AI?\"\n",
        "initial = first_responder.respond({\"messages\": [HumanMessage(example_question)]})\n",
        "initial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2605fd8d-c663-446f-ba25-751190195749",
      "metadata": {},
      "outputs": [],
      "source": [
        "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
        "    - You should use the previous critique to add important information to your answer.\n",
        "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
        "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
        "            - [1] https://example.com\n",
        "            - [2] https://example.com\n",
        "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Extend the initial answer schema to include references.\n",
        "# Forcing citation in the model encourages grounded responses\n",
        "class ReviseAnswer(AnswerQuestion):\n",
        "    \"\"\"Revise your original answer to your question. Provide an answer, reflection,\n",
        "\n",
        "    cite your reflection with references, and finally\n",
        "    add search queries to improve the answer.\"\"\"\n",
        "\n",
        "    references: list[str] = Field(\n",
        "        description=\"Citations motivating your updated answer.\"\n",
        "    )\n",
        "\n",
        "\n",
        "revision_chain = actor_prompt_template.partial(\n",
        "    first_instruction=revise_instructions,\n",
        "    function_name=ReviseAnswer.__name__,\n",
        ") | llm.bind_tools(tools=[ReviseAnswer])\n",
        "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
        "\n",
        "revisor = ResponderWithRetries(\n",
        "    runnable=revision_chain, validator=revision_validator, name=\"Revisor\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "6fd51f17-c0b0-44b6-90e2-55a66cb8f5a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responder::Revisor Attempt 0\n",
            "Responder::Revisor Response is valid\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_nDsGZ5WjdDpkUvG1za6bWVdH', 'function': {'arguments': '{\"answer\":\"Reflection is a crucial aspect of AI for several reasons. Firstly, it allows AI systems to evaluate their own performance and make adjustments to improve accuracy and efficiency. By reflecting on past actions and outcomes, AI can identify patterns and learn from mistakes, leading to continuous improvement. Secondly, reflection helps in debugging and troubleshooting. When an AI system encounters an error or produces an unexpected result, reflection can help trace back the steps and identify the root cause. Thirdly, reflection enhances transparency and accountability. By documenting the decision-making process, AI systems can provide explanations for their actions, which is essential for building trust with users. Lastly, reflection supports ethical AI development. By continuously assessing the impact of their actions, AI systems can ensure they are aligned with ethical guidelines and avoid harmful behaviors.\",\"reflection\":{\"missing\":\"The answer lacks specific examples of AI systems that use reflection and concrete evidence or studies that support the claims made. Including these would strengthen the argument.\",\"superfluous\":\"The answer is concise and to the point, with no superfluous information. Each point made is relevant to the question.\"},\"search_queries\":[\"examples of AI systems using reflection\",\"studies on reflection in AI\",\"importance of reflection in AI development\"],\"references\":[]}', 'name': 'ReviseAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 258, 'prompt_tokens': 522, 'total_tokens': 780}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c5015e65-5943-467b-a523-2a7cc0949d33-0', tool_calls=[{'name': 'ReviseAnswer', 'args': {'answer': 'Reflection is a crucial aspect of AI for several reasons. Firstly, it allows AI systems to evaluate their own performance and make adjustments to improve accuracy and efficiency. By reflecting on past actions and outcomes, AI can identify patterns and learn from mistakes, leading to continuous improvement. Secondly, reflection helps in debugging and troubleshooting. When an AI system encounters an error or produces an unexpected result, reflection can help trace back the steps and identify the root cause. Thirdly, reflection enhances transparency and accountability. By documenting the decision-making process, AI systems can provide explanations for their actions, which is essential for building trust with users. Lastly, reflection supports ethical AI development. By continuously assessing the impact of their actions, AI systems can ensure they are aligned with ethical guidelines and avoid harmful behaviors.', 'reflection': {'missing': 'The answer lacks specific examples of AI systems that use reflection and concrete evidence or studies that support the claims made. Including these would strengthen the argument.', 'superfluous': 'The answer is concise and to the point, with no superfluous information. Each point made is relevant to the question.'}, 'search_queries': ['examples of AI systems using reflection', 'studies on reflection in AI', 'importance of reflection in AI development'], 'references': []}, 'id': 'call_nDsGZ5WjdDpkUvG1za6bWVdH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 522, 'output_tokens': 258, 'total_tokens': 780})"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "revised = revisor.respond(\n",
        "    [\n",
        "        HumanMessage(content=example_question),\n",
        "        initial,\n",
        "        ToolMessage(\n",
        "            tool_call_id=initial.tool_calls[0][\"id\"],\n",
        "            content=json.dumps(\n",
        "                # website_schema_validator_tool.invoke(\n",
        "                #     {\"query\": initial.tool_calls[0][\"args\"][\"reflection\"][\"search_queries\"][0]}\n",
        "                # )\n",
        "                {\"content\": \"This is a test\"}\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "revised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "fccd6a17",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "\n",
        "def run_queries(search_queries: list[str], **kwargs):\n",
        "    \"\"\"Run the generated queries.\"\"\"\n",
        "    print(search_queries)\n",
        "    return website_schema_validator_tool.batch([{\"website_schema\": query} for query in search_queries])\n",
        "\n",
        "\n",
        "tool_node = ToolNode(\n",
        "    [\n",
        "        StructuredTool.from_function(run_queries, name=AnswerQuestion.__name__),\n",
        "        StructuredTool.from_function(run_queries, name=ReviseAnswer.__name__),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "aadf4247",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tools(recurse=True, tools_by_name={'AnswerQuestion': StructuredTool(name='AnswerQuestion', description='Run the generated queries.', args_schema=<class 'pydantic.v1.main.AnswerQuestionSchema'>, func=<function run_queries at 0x7ff295bb58a0>), 'ReviseAnswer': StructuredTool(name='ReviseAnswer', description='Run the generated queries.', args_schema=<class 'pydantic.v1.main.ReviseAnswerSchema'>, func=<function run_queries at 0x7ff295bb58a0>)}, handle_tool_errors=True)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "41b7fb24",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sf']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [ToolMessage(content='[true]', name='AnswerQuestion', tool_call_id='tool_call_id')]}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "message_with_single_tool_call = AIMessage(\n",
        "    content=\"\",\n",
        "    tool_calls=[\n",
        "        {\n",
        "            \"name\": \"AnswerQuestion\",\n",
        "            \"args\": {\"search_queries\": [\"sf\"]},\n",
        "            \"id\": \"tool_call_id\",\n",
        "            \"type\": \"tool_call\",\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "tool_node.invoke({\"messages\": [message_with_single_tool_call]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "3c57318f-a30c-4dbd-9b88-f2633e8cb3b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "MAX_ITERATIONS = 5\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"draft\", first_responder.respond)\n",
        "\n",
        "\n",
        "builder.add_node(\"execute_tools\", tool_node)\n",
        "builder.add_node(\"revise\", revisor.respond)\n",
        "# draft -> execute_tools\n",
        "builder.add_edge(\"draft\", \"execute_tools\")\n",
        "# execute_tools -> revise\n",
        "builder.add_edge(\"execute_tools\", \"revise\")\n",
        "\n",
        "# Define looping logic:\n",
        "\n",
        "\n",
        "def _get_num_iterations(state: list):\n",
        "    i = 0\n",
        "    for m in state[::-1]:\n",
        "        if m.type not in {\"tool\", \"ai\"}:\n",
        "            break\n",
        "        i += 1\n",
        "    return i\n",
        "\n",
        "\n",
        "def event_loop(state: list) -> Literal[\"execute_tools\", \"__end__\"]:\n",
        "    # in our case, we'll just stop after N plans\n",
        "    num_iterations = _get_num_iterations(state)\n",
        "    if num_iterations > MAX_ITERATIONS:\n",
        "        return END\n",
        "    return \"execute_tools\"\n",
        "\n",
        "\n",
        "# revise -> execute_tools OR end\n",
        "builder.add_conditional_edges(\"revise\", event_loop)\n",
        "builder.add_edge(START, \"draft\")\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "7541f82c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "2634a3ea-7423-4579-9f4e-390e439c3209",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responder::First Attempt 0\n",
            "Responder::First Response is invalid\n",
            "Responder::First Attempt 1\n",
            "Responder::First Response is valid\n"
          ]
        },
        {
          "ename": "InvalidUpdateError",
          "evalue": "Must write to at least one of ['messages']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow should we handle the climate crisis?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1333\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1333\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1018\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1018\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1419\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1417\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langchain_core/runnables/base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/utils.py:93\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace:\n\u001b[0;32m---> 93\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     config \u001b[38;5;241m=\u001b[39m merge_configs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, config)\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langchain_core/runnables/base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1784\u001b[0m         Output,\n\u001b[0;32m-> 1785\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1795\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langchain_core/runnables/config.py:397\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    396\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/write.py:107\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    101\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    102\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# write packets and values\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_at_least_one_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.11/site-packages/langgraph/pregel/write.py:157\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, values, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m filtered} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         )\n\u001b[1;32m    160\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m][CONFIG_KEY_SEND]\n\u001b[1;32m    161\u001b[0m write(filtered)\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['messages']"
          ]
        }
      ],
      "source": [
        "graph.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"How should we handle the climate crisis?\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
